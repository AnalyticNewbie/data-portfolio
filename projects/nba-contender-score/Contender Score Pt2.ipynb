{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf46512-903c-4190-9d8f-dbdc9c4fd02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using LLM to continue the analysis\n",
    "Testing the predictive power of changes in the Offensive Raing and Defensive Raing individually to determine which side of the ball has a stonger statistical impact on becoming a contender\n",
    "\n",
    "Running 2 separate Logistic Regression models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b48e07af-dba6-4d7d-9e33-3b0581bbd6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### ðŸ€ Model A: Offense (Rel_ORtg) Impact\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            IsContender   No. Observations:                  420\n",
      "Model:                          Logit   Df Residuals:                      418\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Tue, 25 Nov 2025   Pseudo R-squ.:                  0.2022\n",
      "Time:                        14:35:02   Log-Likelihood:                -125.46\n",
      "converged:                       True   LL-Null:                       -157.27\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.519e-15\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.5438      0.223    -11.390      0.000      -2.982      -2.106\n",
      "Rel_ORtg       0.4132      0.060      6.915      0.000       0.296       0.530\n",
      "==============================================================================\n",
      "\n",
      "### ðŸ›¡ï¸ Model B: Defense (Rel_DRtg) Impact\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            IsContender   No. Observations:                  420\n",
      "Model:                          Logit   Df Residuals:                      418\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Tue, 25 Nov 2025   Pseudo R-squ.:                  0.2161\n",
      "Time:                        14:35:02   Log-Likelihood:                -123.29\n",
      "converged:                       True   LL-Null:                       -157.27\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.667e-16\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -2.5963      0.228    -11.402      0.000      -3.043      -2.150\n",
      "Rel_DRtg      -0.4540      0.064     -7.082      0.000      -0.580      -0.328\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# --- 1. CONFIGURATION (Using the statistically appropriate names) ---\n",
    "FILE_NAME = 'Team Records.csv' \n",
    "TEAM_COL = 'Team'\n",
    "SEASON_COL = 'Season' \n",
    "REL_ORTG_COL = 'Rel_ORtg'  # Used for regression\n",
    "REL_DRTG_COL = 'Rel_DRtg'  # Used for regression\n",
    "PLAYOFF_COL = 'Playoffs'\n",
    "\n",
    "# --- 2. DATA CLEANING & PREP ---\n",
    "df = pd.read_csv(FILE_NAME) \n",
    "\n",
    "# Clean NaNs\n",
    "df.dropna(subset=[REL_ORTG_COL, REL_DRTG_COL], inplace=True)\n",
    "\n",
    "# Create Year Columns\n",
    "df['StartYear'] = df[SEASON_COL].str[:4].astype('Int64')\n",
    "df['FinishYear'] = df['StartYear'] + 1\n",
    "SEASON_FILTER_COL = 'FinishYear'\n",
    "\n",
    "# Filter to 30-team era\n",
    "df_modern = df[df[SEASON_FILTER_COL] >= 2005].copy()\n",
    "\n",
    "# Create IsContender Variable\n",
    "contender_outcomes = ['Finals Winner', 'Finals Loser', 'Conf. Finals'] # Use key strings from the mapping function\n",
    "\n",
    "def map_is_contender(result):\n",
    "    result_str = str(result)\n",
    "    # Check for the key phrases defined in the mapping function\n",
    "    if 'Won Finals' in result_str or 'Lost Finals' in result_str or 'Conf. Finals' in result_str:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df_modern['IsContender'] = df_modern[PLAYOFF_COL].apply(map_is_contender) \n",
    "\n",
    "# --- 3. MODEL A: Offensive Impact (Rel_ORtg) ---\n",
    "\n",
    "X_off = sm.add_constant(df_modern[REL_ORTG_COL])\n",
    "Y = df_modern['IsContender']\n",
    "\n",
    "model_off = sm.Logit(Y, X_off)\n",
    "result_off = model_off.fit(disp=False) \n",
    "\n",
    "print(\"### ðŸ€ Model A: Offense (Rel_ORtg) Impact\")\n",
    "print(result_off.summary().as_text())\n",
    "\n",
    "# --- 4. MODEL B: Defensive Impact (Rel_DRtg) ---\n",
    "\n",
    "X_def = sm.add_constant(df_modern[REL_DRTG_COL])\n",
    "Y = df_modern['IsContender']\n",
    "\n",
    "model_def = sm.Logit(Y, X_def)\n",
    "result_def = model_def.fit(disp=False)\n",
    "\n",
    "print(\"\\n### ðŸ›¡ï¸ Model B: Defense (Rel_DRtg) Impact\")\n",
    "print(result_def.summary().as_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3fd79522-6351-4fee-8f0c-1170ff30b3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defense is a slightly stronger independent predictor of reaching the conference finals\n",
    "# Predictive Power (R^2): Model A: 0.2022, Model B: 0.2161\n",
    "# Defense explains more variability in contender status\n",
    "# Coefficients: Model A: +0.4132, Model B: -0.4540\n",
    "# Defence has a slightly larger magnitude of impact\n",
    "\n",
    "# 1. Predictive Strength (Pseudo R-squared)The $\\text{Pseudo R-squared}$ is your best metric for overall predictive power.\n",
    "# Defense (Model B) has a $\\text{Pseudo R-squared}$ of $0.2161$ ($\\mathbf{21.61\\%}$).\n",
    "# Offense (Model A) has a $\\text{Pseudo R-squared}$ of $0.2022$ ($\\mathbf{20.22\\%}$).\n",
    "\n",
    "# Defense explains about $1.4$ percentage points more of the variability in contender status than Offense does when tested as a single predictor.\n",
    "\n",
    "# 2. Magnitude of Impact (Coefficients)The coefficients show the change in log-odds for a 1-point movement in the rating.\n",
    "# Offense ($\\text{Rel\\_ORtg}$):$\\text{Coefficient} = +0.4132$\n",
    "# Interpretation: A 1-point increase in a team's Offensive Rating (relative to the league average) increases the odds of being a contender by about 1.51 times.\n",
    "# Defense ($\\text{Rel\\_DRtg}$):$\\text{Coefficient} = -0.4540$ (Note: A negative coefficient is good for defense, as a lower rating is better).\n",
    "# Interpretation: A 1-point decrease in a team's Defensive Rating (relative to the league average) increases the odds of being a contender by about 1.57 times ($e^{0.4540}$)\n",
    "\n",
    "# ConclusionThe data supports the basketball maxim: Defense travels in the playoffs. \n",
    "# While both offensive and defensive efficiency are highly significant predictors of success, a single unit of improvement in Defensive Rating provides a slightly larger boost to the probability of reaching the Conference Finals than the equivalent unit of improvement in Offensive Rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ff3d75e-1402-4d70-80e6-b4e5e98b684f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "âœ… Data Prep Complete. Running All Models:\n",
      "\n",
      "## 1. Original Hypothesis: ContenderScore Validity\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            IsContender   No. Observations:                  420\n",
      "Model:                          Logit   Df Residuals:                      418\n",
      "Method:                           MLE   Df Model:                            1\n",
      "Date:                Tue, 25 Nov 2025   Pseudo R-squ.:                  0.3941\n",
      "Time:                        14:39:06   Log-Likelihood:                -95.285\n",
      "converged:                       True   LL-Null:                       -157.27\n",
      "Covariance Type:            nonrobust   LLR p-value:                 8.569e-29\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const            -10.0687      1.223     -8.231      0.000     -12.466      -7.671\n",
      "ContenderScore     1.2055      0.161      7.505      0.000       0.891       1.520\n",
      "==================================================================================\n",
      "\n",
      "## 2. Offense vs. Defense Impact (Comparing Pseudo R-squared)\n",
      "### ðŸ€ Model A: Offense (Rel_ORtg) - Pseudo R-squared:\n",
      "R-squared: 0.2022\n",
      "### ðŸ›¡ï¸ Model B: Defense (Rel_DRtg) - Pseudo R-squared:\n",
      "R-squared: 0.2161\n",
      "\n",
      "## 3. Model Refinement: ContenderScore + EliteBonus\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:            IsContender   No. Observations:                  420\n",
      "Model:                          Logit   Df Residuals:                      417\n",
      "Method:                           MLE   Df Model:                            2\n",
      "Date:                Tue, 25 Nov 2025   Pseudo R-squ.:                  0.4018\n",
      "Time:                        14:39:06   Log-Likelihood:                -94.082\n",
      "converged:                       True   LL-Null:                       -157.27\n",
      "Covariance Type:            nonrobust   LLR p-value:                 3.622e-28\n",
      "==================================================================================\n",
      "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
      "----------------------------------------------------------------------------------\n",
      "const            -11.0919      1.481     -7.489      0.000     -13.995      -8.189\n",
      "ContenderScore     1.3556      0.198      6.834      0.000       0.967       1.744\n",
      "EliteBonus        -1.0631      0.678     -1.568      0.117      -2.392       0.266\n",
      "==================================================================================\n",
      "\n",
      "Final Refined Model Accuracy: 0.8952\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "FILE_NAME = 'Team Records.csv' \n",
    "TEAM_COL = 'Team'\n",
    "SEASON_COL = 'Season' \n",
    "REL_ORTG_COL = 'Rel_ORtg' \n",
    "REL_DRTG_COL = 'Rel_DRtg'\n",
    "PLAYOFF_COL = 'Playoffs'\n",
    "\n",
    "# --- 2. DATA PREP: LOAD, CLEAN, and FEATURE ENGINEERING ---\n",
    "\n",
    "def map_is_contender(result):\n",
    "    result_str = str(result)\n",
    "    # Contender = Conference Finals or better\n",
    "    if 'Won Finals' in result_str or 'Lost Finals' in result_str or 'Conf. Finals' in result_str:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "# Load data \n",
    "df = pd.read_csv(FILE_NAME) \n",
    "\n",
    "# Clean NaNs and filter to 30-team era (2005 onwards)\n",
    "df.dropna(subset=[REL_ORTG_COL, REL_DRTG_COL], inplace=True)\n",
    "df['StartYear'] = df[SEASON_COL].str[:4].astype('Int64')\n",
    "df['FinishYear'] = df['StartYear'] + 1\n",
    "df_modern = df[df['FinishYear'] >= 2005].copy()\n",
    "\n",
    "# Create Target Variable (Y)\n",
    "df_modern['IsContender'] = df_modern[PLAYOFF_COL].apply(map_is_contender) \n",
    "\n",
    "# --- Feature Engineering ---\n",
    "# 1. OffRank & DefRank (REQUIRED FOR CONTENDER SCORE)\n",
    "df_modern['OffRank'] = df_modern.groupby('FinishYear')[REL_ORTG_COL].rank(method='min', ascending=False).astype('Int64')\n",
    "df_modern['DefRank'] = df_modern.groupby('FinishYear')[REL_DRTG_COL].rank(method='min', ascending=True).astype('Int64')\n",
    "\n",
    "# 2. ContenderScore (Based on Rank)\n",
    "df_modern['ContenderScore'] = 10 * ( (30 - df_modern['OffRank']) + (30 - df_modern['DefRank']) ) / 60\n",
    "\n",
    "# 3. EliteBonus (Non-linear Feature - The Fix)\n",
    "ELITE_THRESHOLD = 5 \n",
    "df_modern['EliteBonus'] = np.where(\n",
    "    (df_modern['OffRank'] <= ELITE_THRESHOLD) & (df_modern['DefRank'] <= ELITE_THRESHOLD), \n",
    "    1, \n",
    "    0\n",
    ")\n",
    "\n",
    "# Define Y for all models\n",
    "Y = df_modern['IsContender']\n",
    "\n",
    "print(\"---\")\n",
    "print(\"âœ… Data Prep Complete. Running All Models:\")\n",
    "\n",
    "# ==============================================================================\n",
    "# HYPOTHESIS 1: Original Hypothesis (ContenderScore Validity)\n",
    "# ==============================================================================\n",
    "# X1 = ContenderScore only (Linear Model)\n",
    "X1 = sm.add_constant(df_modern['ContenderScore'].astype(float))\n",
    "model1 = sm.Logit(Y, X1).fit(disp=False)\n",
    "print(\"\\n## 1. Original Hypothesis: ContenderScore Validity\")\n",
    "print(model1.summary().as_text())\n",
    "\n",
    "# ==============================================================================\n",
    "# HYPOTHESIS 2: Offense vs. Defense Impact\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n## 2. Offense vs. Defense Impact (Comparing Pseudo R-squared)\")\n",
    "\n",
    "# Model A: Offensive Impact\n",
    "X2_off = sm.add_constant(df_modern[REL_ORTG_COL].astype(float))\n",
    "model2_off = sm.Logit(Y, X2_off).fit(disp=False)\n",
    "print(\"### ðŸ€ Model A: Offense (Rel_ORtg) - Pseudo R-squared:\")\n",
    "print(f\"R-squared: {model2_off.prsquared:.4f}\")\n",
    "\n",
    "# Model B: Defensive Impact\n",
    "X2_def = sm.add_constant(df_modern[REL_DRTG_COL].astype(float))\n",
    "model2_def = sm.Logit(Y, X2_def).fit(disp=False)\n",
    "print(\"### ðŸ›¡ï¸ Model B: Defense (Rel_DRtg) - Pseudo R-squared:\")\n",
    "print(f\"R-squared: {model2_def.prsquared:.4f}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# HYPOTHESIS 3: Model Refinement (Elite Bonus)\n",
    "# ==============================================================================\n",
    "\n",
    "# X3 = ContenderScore (Linear) + EliteBonus (Non-linear)\n",
    "X3 = df_modern[['ContenderScore', 'EliteBonus']].astype(float)\n",
    "X3 = sm.add_constant(X3)\n",
    "model3 = sm.Logit(Y, X3).fit(disp=False)\n",
    "\n",
    "print(\"\\n## 3. Model Refinement: ContenderScore + EliteBonus\")\n",
    "print(model3.summary().as_text())\n",
    "\n",
    "# Final Accuracy Check on Refined Model\n",
    "predictions_proba = model3.predict(X3)\n",
    "predictions_binary = (predictions_proba >= 0.5).astype(int)\n",
    "accuracy = accuracy_score(Y, predictions_binary)\n",
    "print(f\"\\nFinal Refined Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a27d724-207b-4cdc-a215-b6b962c294f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "ðŸŽ¯ Results with 30% Probability Threshold:\n",
      "New Overall Accuracy: 0.8833\n",
      "New Model Precision: 0.5224\n",
      "New Model Recall: 0.6731\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_score, accuracy_score\n",
    "\n",
    "# Assuming X3, model3, and Y are defined from the final refinement model\n",
    "\n",
    "# Make probability predictions\n",
    "predictions_proba = model3.predict(X3)\n",
    "\n",
    "# 1. ADJUST THE THRESHOLD HERE: Use 0.3 instead of 0.5\n",
    "predictions_binary_low_thresh = (predictions_proba >= 0.3).astype(int)\n",
    "\n",
    "# Calculate new Recall and Precision\n",
    "new_recall = recall_score(Y, predictions_binary_low_thresh)\n",
    "new_precision = precision_score(Y, predictions_binary_low_thresh)\n",
    "new_accuracy = accuracy_score(Y, predictions_binary_low_thresh)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"ðŸŽ¯ Results with 30% Probability Threshold:\")\n",
    "print(f\"New Overall Accuracy: {new_accuracy:.4f}\")\n",
    "print(f\"New Model Precision: {new_precision:.4f}\")\n",
    "print(f\"New Model Recall: {new_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32cbe46c-dde4-42ee-ba97-c459040a33e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "ðŸ”¬ Results for Unconstrained Two-Way Model (Rel_ORtg + Rel_DRtg):\n",
      "Pseudo R-squared: 0.4277\n",
      "Overall Accuracy: 0.8976\n",
      "Model Recall: 0.4423\n",
      "Model Precision: 0.6216\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_modern and Y are defined\n",
    "\n",
    "# X_unconstrained = Rel_ORtg + Rel_DRtg (Unconstrained model)\n",
    "X_unconstrained = df_modern[['Rel_ORtg', 'Rel_DRtg']].astype(float)\n",
    "X_unconstrained = sm.add_constant(X_unconstrained)\n",
    "\n",
    "model_unconstrained = sm.Logit(Y, X_unconstrained).fit(disp=False)\n",
    "\n",
    "# Calculate accuracy scores for the new model\n",
    "predictions_proba_unc = model_unconstrained.predict(X_unconstrained)\n",
    "predictions_binary_unc = (predictions_proba_unc >= 0.5).astype(int)\n",
    "\n",
    "accuracy_unc = accuracy_score(Y, predictions_binary_unc)\n",
    "recall_unc = recall_score(Y, predictions_binary_unc)\n",
    "precision_unc = precision_score(Y, predictions_binary_unc)\n",
    "\n",
    "print(\"\\n---\")\n",
    "print(\"ðŸ”¬ Results for Unconstrained Two-Way Model (Rel_ORtg + Rel_DRtg):\")\n",
    "print(f\"Pseudo R-squared: {model_unconstrained.prsquared:.4f}\")\n",
    "print(f\"Overall Accuracy: {accuracy_unc:.4f}\")\n",
    "print(f\"Model Recall: {recall_unc:.4f}\")\n",
    "print(f\"Model Precision: {precision_unc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "979ba99b-5fa5-4fc3-bda1-591fcedd702b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "## ðŸŒ³ Random Forest Model Performance (70/30 Split)\n",
      "Overall Accuracy: 0.8571\n",
      "Model Precision (Contenders): 0.4000\n",
      "Model Recall (Contenders): 0.2500\n",
      "\n",
      "Confusion Matrix:\n",
      "|                          |   Predicted Non-Contender (0) |   Predicted Contender (1) |\n",
      "|:-------------------------|------------------------------:|--------------------------:|\n",
      "| Actual Non-Contender (0) |                           104 |                         6 |\n",
      "| Actual Contender (1)     |                            12 |                         4 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df_modern and IsContender are available from previous steps.\n",
    "\n",
    "# --- 1. Define Features (X) and Target (Y) ---\n",
    "# Use the continuous relative ratings, allowing the forest to learn non-linear thresholds.\n",
    "# ContenderScore is also included as a synthetic feature.\n",
    "feature_cols = ['Rel_ORtg', 'Rel_DRtg', 'ContenderScore']\n",
    "\n",
    "X = df_modern[feature_cols].astype(float)\n",
    "Y = df_modern['IsContender']\n",
    "\n",
    "# --- 2. Split Data (Training and Testing) ---\n",
    "# We use a standard 70/30 split. Setting a random_state ensures reproducible results.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "# --- 3. Train the Random Forest Model ---\n",
    "# RandomForestClassifier is used for binary classification.\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,  # Number of trees in the forest\n",
    "    max_depth=5,       # Restrict tree depth to prevent severe overfitting\n",
    "    random_state=42\n",
    ")\n",
    "rf_model.fit(X_train, Y_train)\n",
    "\n",
    "# --- 4. Predict and Evaluate ---\n",
    "Y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate key metrics\n",
    "rf_accuracy = accuracy_score(Y_test, Y_pred)\n",
    "rf_precision = precision_score(Y_test, Y_pred)\n",
    "rf_recall = recall_score(Y_test, Y_pred)\n",
    "rf_conf_matrix = confusion_matrix(Y_test, Y_pred)\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "print(\"## ðŸŒ³ Random Forest Model Performance (70/30 Split)\")\n",
    "print(f\"Overall Accuracy: {rf_accuracy:.4f}\")\n",
    "print(f\"Model Precision (Contenders): {rf_precision:.4f}\")\n",
    "print(f\"Model Recall (Contenders): {rf_recall:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(pd.DataFrame(rf_conf_matrix, \n",
    "                   columns=['Predicted Non-Contender (0)', 'Predicted Contender (1)'], \n",
    "                   index=['Actual Non-Contender (0)', 'Actual Contender (1)']).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "caa357a1-37e9-4b39-a1e7-424c1d8a68bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "## ðŸŒ³ Random Forest Model Performance (BALANCED)\n",
      "Overall Accuracy: 0.8730\n",
      "Model Precision (Contenders): 0.5000\n",
      "Model Recall (Contenders): 0.6250\n",
      "\n",
      "Confusion Matrix (Balanced Model):\n",
      "|                          |   Predicted Non-Contender (0) |   Predicted Contender (1) |\n",
      "|:-------------------------|------------------------------:|--------------------------:|\n",
      "| Actual Non-Contender (0) |                           100 |                        10 |\n",
      "| Actual Contender (1)     |                             6 |                        10 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X, Y are defined from the previous steps.\n",
    "# Features: Rel_ORtg, Rel_DRtg, ContenderScore\n",
    "\n",
    "# --- Split Data (Re-run) ---\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.30, random_state=42, stratify=Y\n",
    ")\n",
    "\n",
    "# --- Train the Random Forest Model with Class Weighting ---\n",
    "rf_model_balanced = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    # ðŸ’¥ FIX: ADD THE CLASS WEIGHT PARAMETER ðŸ’¥\n",
    "    class_weight='balanced' \n",
    ")\n",
    "rf_model_balanced.fit(X_train, Y_train)\n",
    "\n",
    "# --- Predict and Evaluate ---\n",
    "Y_pred_balanced = rf_model_balanced.predict(X_test)\n",
    "\n",
    "# Calculate key metrics\n",
    "rf_accuracy_b = accuracy_score(Y_test, Y_pred_balanced)\n",
    "rf_precision_b = precision_score(Y_test, Y_pred_balanced)\n",
    "rf_recall_b = recall_score(Y_test, Y_pred_balanced)\n",
    "rf_conf_matrix_b = confusion_matrix(Y_test, Y_pred_balanced)\n",
    "\n",
    "\n",
    "print(\"---\")\n",
    "print(\"## ðŸŒ³ Random Forest Model Performance (BALANCED)\")\n",
    "print(f\"Overall Accuracy: {rf_accuracy_b:.4f}\")\n",
    "print(f\"Model Precision (Contenders): {rf_precision_b:.4f}\")\n",
    "print(f\"Model Recall (Contenders): {rf_recall_b:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix (Balanced Model):\")\n",
    "print(pd.DataFrame(rf_conf_matrix_b, \n",
    "                   columns=['Predicted Non-Contender (0)', 'Predicted Contender (1)'], \n",
    "                   index=['Actual Non-Contender (0)', 'Actual Contender (1)']).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bde2c1-de99-4039-bb21-b27949486e89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
